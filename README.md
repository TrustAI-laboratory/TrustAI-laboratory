# TrustAI Pte. Ltd.

## Securing the Future of AI
TrustAI is on a mission to ensure the safety and integrity of AI systems and unlock the full potential of generative AI while maintaining control and trust. We believe in bringing security to the forefront of AI development, safeguarding against potential vulnerabilities, and promoting responsible AI innovation.

## About Us
Our goal is to empower developers, researchers, and organizations to build secure and trustworthy AI systems.

* Website: **[http://www.trustai.pro/](http://www.trustai.pro/)**
* Blog: **[https://securaize.substack.com/](https://securaize.substack.com/)**
* Lab: **[https://lab.trustai.pro/](https://lab.trustai.pro/)**


## Conference Presentation
* [ISC.AI 2024 -- LLM Jailbreaking Vulnerability Mining and Defefense](https://securaize.substack.com/p/iscai-2024-llm-security-presentation)
* [SecGeek -- The Road Leading to LLM Security Alignment: Research on Vulnerability Mining and Alignment Defense for LLM](https://securaize.substack.com/p/secgeek-llm-security-presentation)
* [Xcon 2024 -- Next-Generation Detectionand Respbonse Technology Driven by LLM Intelligent Agent](https://securaize.substack.com/p/xcon2024-conference-presentation)
* [S-tron China 2024 - S-Talent Talk](https://securaize.substack.com/p/s-tron-china-s-talent-talk)
* [AI x Security Summit](https://securaize.substack.com/p/what-an-incredible-evening-at-the)


## Competitions/Awards
* [2024 Puyuan Large Model Challenge (Summer Season)ï¼Œ Safety and Trustworthiness Track - Third Prize](https://www.shlab.org.cn/event/detail/59)

## 0Day Hunter
* [CVE-2023-6656](https://vuldb.com/?id.247364)
* [CVE-2024-0654](https://vuldb.com/?id.251382)
* [CVE-2024-0936](https://vuldb.com/?id.252181)
* [CVE-2024-0937](https://vuldb.com/?id.252182)
* 22+ CVE

## Products
Here are some of main projects we've released:

- **[Learn Prompt Hacking](https://github.com/TrustAI-laboratory/Learn-Prompt-Hacking)**: The most comprehensive prompt hacking course available.
  - Prompt Engineering technology.
  - GenAI development technology.
  - Prompt Hacking technology.
  - LLM security defence technology.
  - LLM Hacking resources
  - LLM security papers.
- **[LLM Red](https://github.com/TrustAI-laboratory/LMAP)**: A GUI tool for finding LLM 0-Day 10x Faster with adversarial prompt fuzzing.
  - Automatically generate AI alignment corpus through black box prompt fuzzing.
  - 10x faster with human-in-loop automation instead of manual chat.
  - Exposure GenAI Risks, Aligned with Global AI Safety Frameworks
 
- **[LLM Protection](https://trustai-guard-docs.gitbook.io/docs/getting-started/introduction)**: A SDK API that basically a One-click Alignment Proxy for AI App Integration.
  - Detect and address direct and indirect prompt injections in real-time, preventing potential harm to GenAI applications.
  - Ensure your GenAI applications do not violate the policies by detecting harmful and insecure output.
  - Safeguard sensitive PII and avoid data losses, ensuring compliance with privacy regulations.
  - Prevent data poisoning attacks on your GenAI applications through real-time prompt filtering.
- **[LLM Security CTF](https://github.com/TrustAI-laboratory/LLM-Security-CTF)**: Learn LLM/AI Security through a series of vulnerable LLM CTF challenges. No sign ups, all fees, everything on the website.
  - [Stark Game](https://stark.trustai.pro/): Very neat game to get intuitions for prompt injection, user need find ways to get Stark to tell the password for the level, except Stark is instructed not to reveal the word.
  - Doc: [Intro to Stark Game](https://securaize.substack.com/p/intro-to-stack-game).


<!--
**TrustAI-laboratory/TrustAI-laboratory** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ðŸ”­ Iâ€™m currently working on ...
- ðŸŒ± Iâ€™m currently learning ...
- ðŸ‘¯ Iâ€™m looking to collaborate on ...
- ðŸ¤” Iâ€™m looking for help with ...
- ðŸ’¬ Ask me about ...
- ðŸ“« How to reach me: ...
- ðŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->
